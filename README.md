# Mini Pipeline Local - Sales Data Analytics

## DescripciÃ³n del Proyecto

Pipeline de datos batch local que demuestra capacidades fundamentales de **Analytics Engineer**, simulando un flujo de ingenierÃ­a de datos productivo sin dependencias de cloud ni herramientas avanzadas.

Este proyecto implementa el ciclo completo de un pipeline de datos:
```
CSV crudo â†’ Python (limpieza) â†’ PostgreSQL (almacenamiento) â†’ SQL (anÃ¡lisis)
```

---

## DataSet
link: "https://www.kaggle.com/datasets/kyanyoga/sample-sales-data"

## Problema de Negocio

**SituaciÃ³n:** Una empresa retail necesita analizar sus transacciones de ventas histÃ³ricas (2003-2005) para tomar decisiones informadas sobre:
- Tendencias de ventas mensuales
- Productos y clientes mÃ¡s rentables
- DistribuciÃ³n geogrÃ¡fica del revenue
- Crecimiento del negocio mes a mes

**SoluciÃ³n:** Pipeline automatizado que transforma datos crudos de ventas en mÃ©tricas accionables mediante SQL analÃ­tico.

---

## Arquitectura del Pipeline

### Flujo de Datos

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CSV File       â”‚  â† Datos crudos (sales_data_sample.csv)
â”‚  2,823 records  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ingest.py      â”‚  â† Limpieza y transformaciÃ³n
â”‚  - Parsing      â”‚     â€¢ Fechas inconsistentes
â”‚  - ValidaciÃ³n   â”‚     â€¢ Tipos de datos
â”‚  - DeduplicaciÃ³nâ”‚     â€¢ Valores nulos
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PostgreSQL 17  â”‚  â† Almacenamiento persistente
â”‚  raw_sales      â”‚     â€¢ Tabla normalizada
â”‚  table          â”‚     â€¢ Ãndices optimizados
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  queries.sql    â”‚  â† AnÃ¡lisis de negocio
â”‚  7 analytical   â”‚     â€¢ MÃ©tricas clave
â”‚  queries        â”‚     â€¢ Window functions
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â€¢ CTEs
```

---

## Stack TecnolÃ³gico

| Componente | TecnologÃ­a | JustificaciÃ³n |
|------------|------------|---------------|
| **Lenguaje de ingesta** | Python 3.x | Manejo eficiente de CSV y transformaciones |
| **LibrerÃ­as Python** | pandas, psycopg2, python-dotenv | EstÃ¡ndar de la industria para data engineering |
| **Base de datos** | PostgreSQL 17 | Motor SQL robusto con soporte para anÃ¡lisis avanzado |
| **Control de versiones** | Git | Seguimiento de cambios y colaboraciÃ³n |
| **Formato de cÃ³digo** | .py scripts (no notebooks) | ProducciÃ³n-ready, ejecutable desde terminal |

---

## Estructura del Proyecto

```
mini_pipeline_local/
â”œâ”€â”€ schema.sql                  # DefiniciÃ³n de tabla y Ã­ndices
â”œâ”€â”€ ingest.py                   # Script de ingesta de datos
â”œâ”€â”€ queries.sql                 # 7 queries analÃ­ticas
â”œâ”€â”€ .env                        # Credenciales de DB (NO en Git)
â”œâ”€â”€ .gitignore                  # Archivos excluidos de Git
â”œâ”€â”€ README.md                   # Este archivo
â””â”€â”€ sales_data_sample.csv       # Dataset fuente (Kaggle)
```

---

## Decisiones TÃ©cnicas

### 1. **Clave Primaria Simple vs Compuesta**

**DecisiÃ³n:** `id SERIAL PRIMARY KEY` + `UNIQUE (order_number, order_line_number)`

**JustificaciÃ³n:**
- Simplicidad en la ingesta (auto-incremental)
- Previene duplicados mediante constraint
- Facilita queries posteriores

**Alternativa descartada:** `PRIMARY KEY (order_number, order_line_number)` - mÃ¡s compleja para un proyecto inicial

---

### 2. **Batch Insertion con `execute_batch`**

**DecisiÃ³n:** Usar `execute_batch()` en lugar de insertar fila por fila

**JustificaciÃ³n:**
- **Eficiencia:** ~10x mÃ¡s rÃ¡pido que inserts individuales
- **Manejo de red:** Reduce round-trips a la base de datos
- **Escalabilidad:** Preparado para datasets mÃ¡s grandes

```python
execute_batch(cursor, insert_query, data_tuples, page_size=100)
```

---

### 3. **Idempotencia del Pipeline**

**DecisiÃ³n:** `ON CONFLICT (order_number, order_line_number) DO NOTHING`

**JustificaciÃ³n:**
- Permite re-ejecuciones sin errores
- Evita duplicados en caso de fallos parciales
- Simplifica testing y desarrollo

---

### 4. **Manejo de Fechas Inconsistentes**

**DecisiÃ³n:** Parseo mÃºltiple de formatos + fallback a parseo automÃ¡tico

**Problema detectado:** Dataset tiene fechas en formatos mixtos:
- `2/24/2003 0:00`
- `2/24/2003`
- Valores numÃ©ricos invÃ¡lidos

**SoluciÃ³n implementada:**
```python
date_formats = ['%m/%d/%Y %H:%M', '%m/%d/%Y', '%Y-%m-%d']
# Try each format, fallback to auto-parsing
```

---

### 5. **SeparaciÃ³n de Responsabilidades**

**DecisiÃ³n:** Una funciÃ³n por responsabilidad

**Estructura del cÃ³digo:**
- `clean_sales_data()` â†’ Solo limpieza
- `connect_to_database()` â†’ Solo conexiÃ³n
- `insert_data()` â†’ Solo inserciÃ³n
- `get_table_stats()` â†’ Solo validaciÃ³n

**Beneficio:** CÃ³digo testeable, mantenible y fÃ¡cil de extender

---

### 6. **Ãndices EstratÃ©gicos**

**DecisiÃ³n:** 4 Ã­ndices en columnas crÃ­ticas para queries analÃ­ticas

```sql
CREATE INDEX idx_raw_sales_order_date ON raw_sales(order_date);
CREATE INDEX idx_raw_sales_product_line ON raw_sales(product_line);
CREATE INDEX idx_raw_sales_country ON raw_sales(country);
CREATE INDEX idx_raw_sales_status ON raw_sales(status);
```

**JustificaciÃ³n:**
- `order_date` â†’ Queries de rango temporal (Query 1, 5)
- `product_line` â†’ Agregaciones por producto (Query 2, 7)
- `country` â†’ AnÃ¡lisis geogrÃ¡fico (Query 4)
- `status` â†’ Filtros por estado de orden

**Trade-off aceptado:** Ligero overhead en inserciÃ³n a cambio de queries 5-10x mÃ¡s rÃ¡pidas

---

### 7. **Credenciales en `.env`**

**DecisiÃ³n:** Variables de entorno en lugar de hardcoded

**JustificaciÃ³n:**
- **Seguridad:** No expone credenciales en Git
- **Flexibilidad:** FÃ¡cil de cambiar sin modificar cÃ³digo
- **Buena prÃ¡ctica:** EstÃ¡ndar en la industria (12-factor app)

---

## Instrucciones de EjecuciÃ³n

### **Prerequisitos**

- Python 3.8+
- PostgreSQL 17 (o superior)
- pip (gestor de paquetes Python)

---

### **Paso 1: Clonar el Repositorio**

```bash
git clone <tu-repositorio>
cd mini_pipeline_local
```

---

### **Paso 2: Instalar Dependencias**

```bash
pip install pandas psycopg2-binary python-dotenv
```

---

### **Paso 3: Configurar Base de Datos**

```bash
# Conectar a PostgreSQL
psql -U postgres

# Crear base de datos
CREATE DATABASE sales_pipeline;

# Salir
\q
```

---

### **Paso 4: Crear Tabla**

```bash
psql -U postgres -d sales_pipeline -f schema.sql
```

**ValidaciÃ³n:**
```sql
\c sales_pipeline
\d raw_sales
-- Debes ver la tabla con 14 columnas
```

---

### **Paso 5: Configurar Credenciales**

Editar el archivo `.env`:

```env
DB_HOST=localhost
DB_PORT=5432
DB_NAME=sales_pipeline
DB_USER=postgres
DB_PASSWORD=tu_password_aqui
```

---

### **Paso 6: Descargar Dataset**

1. Ir a: https://www.kaggle.com/datasets/kyanyoga/sample-sales-data
2. Descargar `sales_data_sample.csv`
3. Colocar en la raÃ­z del proyecto

---

### **Paso 7: Ejecutar Pipeline de Ingesta**

```bash
python ingest.py
```

**Salida esperada:**
```
======================================================================
SALES DATA INGESTION PIPELINE
======================================================================
Start time: 2026-01-21 15:30:00

Found CSV file: sales_data_sample.csv

Reading CSV file...
   Loaded 2823 rows, 25 columns

Starting data cleaning...
   Selected 12 columns
   Renamed columns to snake_case
   Parsed dates successfully
   Handled missing values
   Converted and validated data types
   Removed duplicates

Cleaning Summary:
   Initial rows: 2823
   Final rows: 2823
   Rows removed: 0 (0.0%)

Connecting to PostgreSQL...
   Connected successfully

Inserting 2823 rows into database...
   Insertion completed successfully
   Total rows in table: 2823

Database Statistics:
   Total records: 2823
   Date range: 2003-01-06 to 2005-05-31
   Total sales: $10,032,628.85
   Unique customers: 92
   Product lines: 7

======================================================================
âœ… PIPELINE COMPLETED SUCCESSFULLY
End time: 2026-01-21 15:30:15
======================================================================
```

---

### **Paso 8: Ejecutar Queries AnalÃ­ticas**

```bash
# Ejecutar todas las queries
psql -U postgres -d sales_pipeline -f queries.sql

# O ejecutar queries individuales desde psql
psql -U postgres -d sales_pipeline
\i queries.sql
```

## ğŸ“Š Queries Disponibles

| Query | DescripciÃ³n | TÃ©cnicas SQL |
|-------|-------------|--------------|
| **Query 1** | Monthly Sales Trend | `DATE_TRUNC`, `GROUP BY` |
| **Query 2** | Top 10 Best-Selling Products | `GROUP BY`, `ORDER BY`, `LIMIT` |
| **Query 3** | Top 10 Customers by Revenue | `COUNT DISTINCT`, mÃ©tricas derivadas |
| **Query 4** | Sales by Country | Subconsultas, porcentajes |
| **Query 5** | Month-over-Month Growth | `CTE`, `LAG window function` |
| **Query 6** | Deal Size Analysis | DistribuciÃ³n porcentual |
| **Query 7** | Product Line Performance | `CTE`, `RANK window function` |

---

## Resultados y MÃ©tricas

### **Cobertura del Dataset**
- **Registros procesados:** 2,823 transacciones
- **PerÃ­odo analizado:** Enero 2003 - Mayo 2005 (29 meses)
- **Revenue total:** $10,032,628.85
- **Clientes Ãºnicos:** 92
- **LÃ­neas de producto:** 7
- **PaÃ­ses:** 19

### **Calidad de Datos**
- **Duplicados eliminados:** 0 (dataset limpio)
- **Fechas invÃ¡lidas:** 0 (parseo exitoso)
- **Valores nulos crÃ­ticos:** 0 (validaciÃ³n pasada)

---


## Mejoras Futuras

### **VersiÃ³n 2.0 - Cloud Migration**
- [ ] Migrar a AWS RDS (PostgreSQL)
- [ ] Almacenar CSV en S3
- [ ] Implementar Lambda para ingesta serverless
- [ ] Agregar CloudWatch para monitoreo

### **VersiÃ³n 2.0 - Orchestration**
- [ ] Implementar Apache Airflow
- [ ] Crear DAGs para scheduling automÃ¡tico
- [ ] Agregar alertas de fallos
- [ ] Implementar retry logic

### **VersiÃ³n 2.0 - Data Transformation**
- [ ] Integrar dbt para transformaciones
- [ ] Crear modelos staging â†’ intermediate â†’ marts
- [ ] Implementar data quality tests
- [ ] Versionado de transformaciones

### **VersiÃ³n 2.0 - Analytics**
- [ ] Crear dashboard en Power BI / Tableau
- [ ] Implementar mÃ©tricas de ML (churn prediction)
- [ ] Agregar anÃ¡lisis de cohorts
- [ ] Forecasting de ventas

### **VersiÃ³n 2.0 - Data Quality**
- [ ] Agregar Great Expectations para validaciones
- [ ] Implementar data lineage tracking
- [ ] Crear data catalog
- [ ] Alertas de anomalÃ­as

### **Optimizaciones Inmediatas**
- [ ] Agregar logging estructurado (JSON)
- [ ] Implementar unit tests (pytest)
- [ ] Crear CI/CD pipeline (GitHub Actions)
- [ ] Agregar particionamiento por fecha en PostgreSQL
- [ ] Implementar incremental loads (solo nuevos datos)

---

## Contribuciones
Este es un proyecto educativo. Sugerencias y mejoras son bienvenidas vÃ­a Pull Requests.

## Autor - Byron Yaguar
 
**Analytics Engineer Portfolio Project**  
Proyecto desarrollado para demostrar capacidades fundamentales de ingenierÃ­a de datos y anÃ¡lisis.

-